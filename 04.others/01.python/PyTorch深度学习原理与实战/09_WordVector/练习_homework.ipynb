{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于词向量的英汉翻译——“火炬上的深度学习\"下第一次作业\n",
    "\n",
    "在这个作业中，你需要半独立地完成一个英文到中文的单词翻译器\n",
    "\n",
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第VI课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载必要的程序包\n",
    "# PyTorch的程序包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 数值运算和绘图的程序包\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "# 加载机器学习的软件包，主要为了词向量的二维可视化\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#加载Word2Vec的软件包\n",
    "import gensim as gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "#加载正则表达式处理的包\n",
    "import re\n",
    "\n",
    "#在Notebook界面能够直接显示图形\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：加载词向量\n",
    "\n",
    "首先，让我们加载别人已经在大型语料库上训练好的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载中文词向量，下载地址为：链接：http://pan.baidu.com/s/1gePQAun 密码：kvtg\n",
    "# 该中文词向量库是由尹相志提供，训练语料来源为：微博、人民日报、上海热线、汽车之家等，包含1366130个词向量\n",
    "#word_vectors = KeyedVectors.load_word2vec_format('vectors.bin', binary=True, unicode_errors='ignore')\n",
    "#len(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载中文的词向量，下载地址为：http://nlp.stanford.edu/data/glove.6B.zip，解压后将glove.6B.100d.txt文件拷贝到与本notebook\n",
    "# 文件一致的文件夹洗面。\n",
    "#f = open('glove.6B.100d.txt', 'r')\n",
    "#i = 1\n",
    "\n",
    "# 将英文的词向量都存入如下的字典中\n",
    "#word_vectors_en = {}\n",
    "#with open('glove.6B.100d.txt') as f:\n",
    "#    for line in f:\n",
    "#        numbers = line.split()\n",
    "#        word = numbers[0]\n",
    "#        vectors = np.array([float(i) for i in numbers[1 : ]])\n",
    "#        word_vectors_en[word] = vectors\n",
    "#        i += 1\n",
    "#print(len(word_vectors_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：可视化同一组意思词在两种不同语言的词向量中的相互位置关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文的一二三四五列表\n",
    "#cn_list = {'一', '二', '三', '四', '五', '六', '七', '八', '九', '零'}\n",
    "# 阿拉伯数字的12345列表\n",
    "#en_list = {'1', '2', '3', '4', '5', '6', '7', '8', '9', '0'}\n",
    "# 英文数字的列表\n",
    "#en_list = {'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero'}\n",
    "\n",
    "# 对应词向量都存入到列表中\n",
    "#cn_vectors = []  #中文的词向量列表\n",
    "#en_vectors = []  #英文的词向量列表\n",
    "#for w in cn_list:\n",
    "#    cn_vectors.append(word_vectors[w])\n",
    "#for w in en_list:\n",
    "#    en_vectors.append(word_vectors_en[w])\n",
    "\n",
    "# 将这些词向量统一转化为矩阵\n",
    "#cn_vectors = np.array(cn_vectors)\n",
    "#en_vectors = np.array(en_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降维实现可视化\n",
    "#X_reduced = PCA(n_components=2).fit_transform(cn_vectors)\n",
    "#Y_reduced = PCA(n_components = 2).fit_transform(en_vectors)\n",
    "\n",
    "# 绘制所有单词向量的二维空间投影\n",
    "#f, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 8))\n",
    "#ax1.plot(X_reduced[:, 0], X_reduced[:, 1], 'o')\n",
    "#ax2.plot(Y_reduced[:, 0], Y_reduced[:, 1], 'o')\n",
    "#zhfont1 = matplotlib.font_manager.FontProperties(fname='/Library/Fonts/华文仿宋.ttf', size=16)\n",
    "#for i, w in enumerate(cn_list):\n",
    "#    ax1.text(X_reduced[i, 0], X_reduced[i, 1], w, fontproperties = zhfont1, alpha = 1)\n",
    "#for i, w in enumerate(en_list):\n",
    "#    ax2.text(Y_reduced[i, 0], Y_reduced[i, 1], w, alpha = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论：可以看出，中文的一、二、等数字彼此之间的关系与英文的数字彼此之间的关系很类似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：训练一个神经网络，输入一个英文单词的词向量，输出一个中文的词向量，并翻译为中文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，读入一个已经建立好的词典（dictionary.txt）。本词典是老师调用百度翻译的API，自动将一篇英文小说中的词汇逐个翻译为中文而得来的\n",
    "\n",
    "我们一个个地载入词典，并查找对应的中文词向量，如果找得到，则放入original_words中，做为正式的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_words = []\n",
    "#with open('dictionary.txt', 'r') as f:\n",
    "#    dataset = []\n",
    "#    for line in f:\n",
    "#        itm = line.split('\\t')\n",
    "#        eng = itm[0]\n",
    "#        chn = itm[1].strip()\n",
    "#        if eng in word_vectors_en and chn in word_vectors:\n",
    "#            data = word_vectors_en[eng]\n",
    "#            target = word_vectors[chn]\n",
    "            # 将中英文词对做成数据集\n",
    "#            dataset.append([data, target])\n",
    "#            original_words.append([eng, chn])\n",
    "#print(len(dataset)) # 共有4962个单词做为总的数据集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立训练集、测试集和校验集\n",
    "# 训练集用来训练神经网络，更改网络的参数；校验集用来判断网络模型是否过拟合：当校验集的损失数值超过训练集的时候，即为过拟合\n",
    "# 测试集用来检验模型的好坏\n",
    "#indx = np.random.permutation(range(len(dataset)))\n",
    "#dataset = [dataset[i] for i in indx]\n",
    "#original_words = [original_words[i] for i in indx]\n",
    "#train_size = 500\n",
    "#train_data = dataset[train_size:]\n",
    "#valid_data = dataset[train_size // 2 : train_size]\n",
    "#test_data = dataset[: train_size // 2]\n",
    "#test_words = original_words[: train_size // 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 开始训练一个多层神经网络，将一个100维度的英文向量映射为200维度的中文词向量，隐含层节点为30\n",
    "\n",
    "#input_size = 100\n",
    "#output_size = 200\n",
    "#hidden_size = 30\n",
    "\n",
    "# 新建一个神经网络，包含一个隐含层\n",
    "#model = nn.Sequential(nn.Linear(input_size, hidden_size)\n",
    "#                     nn.Tanh()\n",
    "#                     nn.Linear(hidden_size, output_size)\n",
    "#                     )\n",
    "\n",
    "# 构造损失函数\n",
    "#criterion = torch.nn.MSELoss()\n",
    "\n",
    "# 构造优化器\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "# 总的循环周期\n",
    "#num_epoch = 100\n",
    "\n",
    "\n",
    "#开始训练500次，每次对所有的数据都做循环\n",
    "#results = []\n",
    "#for epoch in range(num_epoch):\n",
    "#    train_loss = []\n",
    "#    for data in train_data:\n",
    "        # 读入数据\n",
    "#        x = Var(torch.FloatTensor(data[0])).unsqueeze(0)\n",
    "#        y = Var(torch.FloatTensor(data[1])).unsqueeze(0)\n",
    "        # 模型预测\n",
    "#        output = model(x)\n",
    "        \n",
    "        # 反向传播算法训练\n",
    "#        optimizer.zero()\n",
    "#        loss = criteri(output, y)\n",
    "#        train_loss.append(loss.data.numpy()[0])\n",
    "#        loss.backwerd()\n",
    "#        optimizer.step()\n",
    "    # 在校验集上测试一下效果\n",
    "#    valid_loss = []\n",
    "#    for data in valid_data:\n",
    "#        x = Var(torch.FloatTensor(data[0])).unsqueeze(0)\n",
    "#        y = Var(torch.FloatTensor(data[1])).unsqueeze(0)\n",
    "#        output = model(x)\n",
    "#        loss = criterion(output, y)\n",
    "#        valid_loss.append(loss.data.numpy()[0])\n",
    "#    results.append([np.mean(train_loss), np.mean(valid_loss)])\n",
    "#    print('{}轮，训练Loss: {:.2f}, 校验Loss: {:.2f}'.format(epoch, np.mean(train_loss), np.mean(valid_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制图形\n",
    "#a = [i[0] for i in results]\n",
    "#b = [i[1] for i in results]\n",
    "#plt.plot(a, 'o', label = 'Training Loss')\n",
    "#plt.plot(b, 's', label = 'Validation Loss')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('Loss Function')\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上验证准确度\n",
    "# 检验标准有两个：一个是直接用预测的词和标准答案做全词匹配；另一个是做单字的匹配\n",
    "#exact_same = 0  #全词匹配数量\n",
    "#one_same = 0 #单字匹配数量\n",
    "#results = []\n",
    "#for i, data in enumerate(test_data):\n",
    "#    x = Var(torch.FloatTensor(data[0])).unsqueeze(0)\n",
    "    # 给出模型的输出\n",
    "#    output = model(x)\n",
    "#    output = output.squeeze().data.numpy()\n",
    "    # 从中文词向量中找到与输出向量最相似的向量\n",
    "#    most_similar = word_vectors.wv.similar_by_vector(output, 1)\n",
    "    # 将标准答案中的词与最相似的向量所对应的词打印出来\n",
    "#    results.append([original_words[i][1], most_similar[0][0]])\n",
    "    \n",
    "    # 全词匹配\n",
    "#    if original_words[i][1] == most_similar[0][0]:\n",
    "#        exact_same += 1\n",
    "    # 某一个字匹配\n",
    "#    if list(set(list(original_words[i][1])) & set(list(most_similar[0][0]))) != []:\n",
    "#        one_same += 1\n",
    "    \n",
    "#print(\"精确匹配率：{:.2f}\".format(1.0 * exact_same / len(test_data)))\n",
    "#print('一字匹配率：{:.2f}'.format(1.0 * one_same / len(test_data)))\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
