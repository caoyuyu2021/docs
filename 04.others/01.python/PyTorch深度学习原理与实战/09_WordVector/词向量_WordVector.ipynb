{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词汇的星空——词向量（Word Vector）技术\n",
    "\n",
    "在这节课中，我们学习了如何通过人工神经网络得到单词的向量表达。\n",
    "\n",
    "首先，我们构建了一个简单的NGram语言模型，根据N个历史词汇预测下一个单词，从而得到每一个单词的向量表示。我们用小说《三体》为例，展示了我们的词向量嵌入效果。\n",
    "\n",
    "其次，我们学习了如何使用成熟的Google开发的Word2Vec包来进行大规模语料的词向量训练，以及如何加载已经训练好的词向量，从而利用这些词向量来做一些简单的运算和测试。\n",
    "\n",
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第VI课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载必要的程序包\n",
    "# PyTorch的程序包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 数值运算和绘图的程序包\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "# 加载机器学习的软件包\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#加载Word2Vec的软件包\n",
    "import gensim as gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "#加载‘结巴’中文分词软件包\n",
    "\n",
    "import jieba\n",
    "\n",
    "#加载正则表达式处理的包\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、NGram 词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGram词向量模型的原理是利用一个人工神经网络来根据前N个单词来预测下一个单词，从而得到每个单词的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 文本预处理\n",
    "\n",
    "我们以刘慈欣著名的科幻小说《三体》为例，来展示利用NGram模型训练词向量的方法\n",
    "\n",
    "预处理分为两个步骤：\n",
    "\n",
    "1、读取文件\n",
    "\n",
    "2、分词\n",
    "\n",
    "3、将语料划分为N＋1元组，准备好训练用数据\n",
    "\n",
    "在这里，我们并没有去除标点符号，一是为了编程简洁，而是考虑到分词会自动将标点符号当作一个单词处理，因此不需要额外考虑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1). 读入原始文件，筛掉所有标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#读入原始文件\n",
    "\n",
    "f = open(\"三体.txt\", 'r')\n",
    "# 若想加快运行速度，使用下面的语句（选用了三体的其中一章）：\n",
    "#f = open(\"3body.txt\", 'r') \n",
    "text = str(f.read())\n",
    "f.close()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)、分词，并去掉标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 分词\n",
    "temp = jieba.lcut(text)\n",
    "words = []\n",
    "for i in temp:\n",
    "    #过滤掉所有的标点符号\n",
    "    i = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\'“”《》?“]+|[+——！，。？、~@#￥%……&*（）：]+\", \"\", i)\n",
    "    if len(i) > 0:\n",
    "        words.append(i)\n",
    "print(len(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)、构建N+1元组作为训练数据对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建三元组列表.  每一个元素为： ([ i-2位置的词, i-1位置的词 ], 下一个词)\n",
    "# 我们选择的Ngram中的N，即窗口大小为2\n",
    "trigrams = [([words[i], words[i + 1]], words[i + 2]) for i in range(len(words) - 2)]\n",
    "# 打印出前三个元素看看\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4). 将每个单词进行编码，构造词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到词汇表\n",
    "vocab = set(words)\n",
    "print(len(vocab))\n",
    "# 两个字典，一个根据单词索引其编号，一个根据编号索引单词\n",
    "#word_to_idx中的值包含两部分，一部分为id，另一部分为单词出现的次数\n",
    "#word_to_idx中的每一个元素形如：{w:[id, count]}，其中w为一个词，id为该词的编号，count为该单词在words全文中出现的次数\n",
    "word_to_idx = {} \n",
    "idx_to_word = {}\n",
    "ids = 0\n",
    "\n",
    "#对全文循环，构建这两个字典\n",
    "for w in words:\n",
    "    cnt = word_to_idx.get(w, [ids, 0])\n",
    "    if cnt[1] == 0:\n",
    "        ids += 1\n",
    "    cnt[1] += 1\n",
    "    word_to_idx[w] = cnt\n",
    "    idx_to_word[ids] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、构造模型并训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1). 构造NGram神经网络模型\n",
    "\n",
    "我们构造了一个三层的网络：\n",
    "\n",
    "1、输入层：embedding层，这一层的作用是：先将输入单词的编号映射为一个one hot编码的向量，形如：001000，维度为单词表大小。\n",
    "然后，embedding会通过一个线性的神经网络层映射到这个词的向量表示，输出为embedding_dim\n",
    "\n",
    "2、线性层，从embedding_dim维度到128维度，然后经过非线性ReLU函数\n",
    "\n",
    "3、线性层：从128维度到单词表大小维度，然后log softmax函数，给出预测每个单词的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  #嵌入层\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128) #线性层\n",
    "        self.linear2 = nn.Linear(128, vocab_size) #线性层\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #嵌入运算，嵌入运算在内部分为两步：将输入的单词编码映射为one hot向量表示，然后经过一个线性层得到单词的词向量\n",
    "        #inputs的尺寸为：1*context_size\n",
    "        embeds = self.embeddings(inputs)\n",
    "        #embeds的尺寸为: context_size*embedding_dim\n",
    "        embeds = embeds.view(1, -1)\n",
    "        #此时embeds的尺寸为：1*embedding_dim\n",
    "        # 线性层加ReLU\n",
    "        out = self.linear1(embeds)\n",
    "        out = F.relu(out)\n",
    "        #此时out的尺寸为1*128\n",
    "        \n",
    "        # 线性层加Softmax\n",
    "        out = self.linear2(out)\n",
    "        #此时out的尺寸为：1*vocab_size\n",
    "        log_probs = F.log_softmax(out, dim = 1)\n",
    "        return log_probs\n",
    "    def extract(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2). 开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [] #纪录每一步的损失函数\n",
    "criterion = nn.NLLLoss() #运用负对数似然函数作为目标函数（常用于多分类问题的目标函数）\n",
    "model = NGram(len(vocab), 10, 2) #定义NGram模型，向量嵌入维数为10维，N（窗口大小）为2\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001) #使用随机梯度下降算法作为优化器\n",
    "\n",
    "#循环100个周期\n",
    "for epoch in range(20):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # 准备好输入模型的数据，将词汇映射为编码\n",
    "        context_idxs = [word_to_idx[w][0] for w in context]\n",
    "        \n",
    "        context_var = torch.tensor(context_idxs, dtype = torch.long)\n",
    "\n",
    "        # 清空梯度：注意PyTorch会在调用backward的时候自动积累梯度信息，故而每隔周期要清空梯度信息一次。\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 用神经网络做计算，计算得到输出的每个单词的可能概率对数值\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        loss = criterion(log_probs, torch.tensor([word_to_idx[target][0]], dtype = torch.long))\n",
    "\n",
    "        # 梯度反传\n",
    "        loss.backward()\n",
    "        \n",
    "        # 对网络进行优化\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 累加损失函数值\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "    print('第{}轮，损失函数为：{:.2f}'.format(epoch, total_loss.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 结果展示\n",
    "#### 1). 将向量投影到二维平面进行可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 从训练好的模型中提取每个单词的向量\n",
    "vec = model.extract(torch.tensor([v[0] for v in word_to_idx.values()], dtype = torch.long))\n",
    "vec = vec.data.numpy()\n",
    "\n",
    "# 利用PCA算法进行降维\n",
    "X_reduced = PCA(n_components=2).fit_transform(vec)\n",
    "\n",
    "\n",
    "# 绘制所有单词向量的二维空间投影\n",
    "fig = plt.figure(figsize = (30, 20))\n",
    "ax = fig.gca()\n",
    "ax.set_facecolor('white')\n",
    "ax.plot(X_reduced[:, 0], X_reduced[:, 1], '.', markersize = 1, alpha = 0.4, color = 'black')\n",
    "\n",
    "\n",
    "# 绘制几个特殊单词的向量\n",
    "words = ['智子', '地球', '三体', '质子', '科学', '世界', '文明', '太空', '加速器', '平面', '宇宙', '信息']\n",
    "\n",
    "# 设置中文字体，否则无法在图形上显示中文\n",
    "zhfont1 = matplotlib.font_manager.FontProperties(fname='./华文仿宋.ttf', size=16)\n",
    "for w in words:\n",
    "    if w in word_to_idx:\n",
    "        ind = word_to_idx[w][0]\n",
    "        xy = X_reduced[ind]\n",
    "        plt.plot(xy[0], xy[1], '.', alpha =1, color = 'red')\n",
    "        plt.text(xy[0], xy[1], w, fontproperties = zhfont1, alpha = 1, color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2). 临近词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义计算cosine相似度的函数\n",
    "def cos_similarity(vec1, vec2):\n",
    "    \n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    norm = norm1 * norm2\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    result = dot / norm if norm > 0 else 0\n",
    "    return result\n",
    "    \n",
    "# 在所有的词向量中寻找到与目标词（word）相近的向量，并按相似度进行排列\n",
    "def find_most_similar(word, vectors, word_idx):\n",
    "    vector = vectors[word_to_idx[word][0]]\n",
    "    simi = [[cos_similarity(vector, vectors[num]), key] for num, key in enumerate(word_idx.keys())]\n",
    "    sort = sorted(simi)[::-1]\n",
    "    words = [i[1] for i in sort]\n",
    "    return words\n",
    "\n",
    "# 与智子靠近的词汇\n",
    "find_most_similar('智子', vec, word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec是Google推出的一个开源的词向量计算工具，它被内嵌到了gensim软件包里。\n",
    "在本课程中，我们主要展示了自己调用Word2Vec训练词向量和读取已经训练好的词向量两种方法，并展示了如何利用词向量来进行一些简单的推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 自己训练一个小语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入文件、分词，形成一句一句的语料\n",
    "# 注意跟前面处理不一样的地方在于，我们一行一行地读入文件，从而自然利用行将文章分开成“句子”\n",
    "f = open(\"三体.txt\", 'r')\n",
    "lines = []\n",
    "for line in f:\n",
    "    temp = jieba.lcut(line)\n",
    "    words = []\n",
    "    for i in temp:\n",
    "        #过滤掉所有的标点符号\n",
    "        i = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\'””《》]+|[+——！，。？、~@#￥%……&*（）：；‘]+\", \"\", i)\n",
    "        if len(i) > 0:\n",
    "            words.append(i)\n",
    "    if len(words) > 0:\n",
    "        lines.append(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用Word2Vec的算法进行训练。\n",
    "# 参数分别为：size: 嵌入后的词向量维度；window: 上下文的宽度，min_count为考虑计算的单词的最低词频阈值\n",
    "model = Word2Vec(lines, size = 20, window = 2 , min_count = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('智子', topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将词向量投影到二维空间\n",
    "rawWordVec = []\n",
    "word2ind = {}\n",
    "for i, w in enumerate(model.wv.vocab):\n",
    "    rawWordVec.append(model[w])\n",
    "    word2ind[w] = i\n",
    "rawWordVec = np.array(rawWordVec)\n",
    "X_reduced = PCA(n_components=2).fit_transform(rawWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制星空图\n",
    "# 绘制所有单词向量的二维空间投影\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "ax = fig.gca()\n",
    "ax.set_facecolor('white')\n",
    "ax.plot(X_reduced[:, 0], X_reduced[:, 1], '.', markersize = 1, alpha = 0.3, color = 'black')\n",
    "\n",
    "\n",
    "# 绘制几个特殊单词的向量\n",
    "words = ['智子', '地球', '三体', '质子', '科学', '世界', '文明', '太空', '加速器', '平面', '宇宙', '进展','的']\n",
    "\n",
    "# 设置中文字体，否则无法在图形上显示中文\n",
    "zhfont1 = matplotlib.font_manager.FontProperties(fname='./华文仿宋.ttf', size=16)\n",
    "for w in words:\n",
    "    if w in word2ind:\n",
    "        ind = word2ind[w]\n",
    "        xy = X_reduced[ind]\n",
    "        plt.plot(xy[0], xy[1], '.', alpha =1, color = 'green')\n",
    "        plt.text(xy[0], xy[1], w, fontproperties = zhfont1, alpha = 1, color = 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、调用一个现成的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). 大规模词向量可视化\n",
    "\n",
    "该中文词向量库是由尹相志提供，训练语料来源为：微博、人民日报、上海热线、汽车之家等，包含1366130个词向量，\n",
    "下载地址为：链接：http://pan.baidu.com/s/1gePQAun 密码：kvtg\n",
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第VI课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载词向量\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./vectors.bin', binary=True, unicode_errors='ignore')\n",
    "len(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA降维\n",
    "rawWordVec = []\n",
    "word2ind = {}\n",
    "for i, w in enumerate(word_vectors.vocab):\n",
    "    rawWordVec.append(word_vectors[w])\n",
    "    word2ind[w] = i\n",
    "rawWordVec = np.array(rawWordVec)\n",
    "X_reduced = PCA(n_components=2).fit_transform(rawWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看相似词\n",
    "word_vectors.most_similar('物理', topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制星空图\n",
    "# 绘制所有的词汇\n",
    "fig = plt.figure(figsize = (30, 15))\n",
    "ax = fig.gca()\n",
    "ax.set_facecolor('black')\n",
    "ax.plot(X_reduced[:, 0], X_reduced[:, 1], '.', markersize = 1, alpha = 0.1, color = 'white')\n",
    "\n",
    "ax.set_xlim([-12,12])\n",
    "ax.set_ylim([-10,20])\n",
    "\n",
    "\n",
    "# 选择几个特殊词汇，不仅画它们的位置，而且把它们的临近词也画出来\n",
    "words = {'徐静蕾','吴亦凡','物理','红楼梦','量子'}\n",
    "all_words = []\n",
    "for w in words:\n",
    "    lst = word_vectors.most_similar(w)\n",
    "    wds = [i[0] for i in lst]\n",
    "    metrics = [i[1] for i in lst]\n",
    "    wds = np.append(wds, w)\n",
    "    all_words.append(wds)\n",
    "\n",
    "\n",
    "zhfont1 = matplotlib.font_manager.FontProperties(fname='./华文仿宋.ttf', size=16)\n",
    "colors = ['red', 'yellow', 'orange', 'green', 'cyan', 'cyan']\n",
    "for num, wds in enumerate(all_words):\n",
    "    for w in wds:\n",
    "        if w in word2ind:\n",
    "            ind = word2ind[w]\n",
    "            xy = X_reduced[ind]\n",
    "            plt.plot(xy[0], xy[1], '.', alpha =1, color = colors[num])\n",
    "            plt.text(xy[0], xy[1], w, fontproperties = zhfont1, alpha = 1, color = colors[num])\n",
    "plt.savefig('88.png',dpi =600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2). 类比关系实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 女人－男人＝？－国王\n",
    "words = word_vectors.most_similar(positive=['女人', '国王'], negative=['男人'])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 北京－中国＝？－俄罗斯\n",
    "words = word_vectors.most_similar(positive=['北京', '俄罗斯'], negative=['中国'])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自然科学－物理学＝？－政治学\n",
    "words = word_vectors.most_similar(positive=['自然科学', '政治学'], negative=['物理学'])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 王菲－章子怡＝？－汪峰\n",
    "words = word_vectors.most_similar(positive=['王菲', '汪峰'], negative=['章子怡'])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尽可能多地选出所有的货币\n",
    "words = word_vectors.most_similar(positive=['美元', '英镑'], topn = 100)\n",
    "words = word_vectors.most_similar(positive=['美元', '英镑', '日元'], topn = 100)\n",
    "#words = word_vectors.most_similar(positive=['美元', '英镑', '日元'], negative = ['原油价格', '7800万'], topn = 100)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
